WEB SCRAPING
date: 09/02/26	
Topics done: 
Intro to web scraping; defs, applications, tools, techniques, components-crawlers and scrapers, working of the crawlers and scrapers, best practices, challenges, future. 
=====================================================================
	
Introduction to web scraping 

Manual copying is slow, manual and time consuming.


What is web scraping: An automated technique using software to extract large amounts of data from websites quickly. data from html to csv or json like structured formats.

Real world applications: 
finance - news data and real time stock prices 
Online Shopping - real time price setting based on competitor prices
cheapest flight prices in real time 
Machine learning models can be trained on the huge amounts of data scraped from web
search engine optimization: keyword rank tracking 
social media trend analysis

Techniques and tools
Manual : copy pasting and it's inefficient and static
key automated techniques
best case scenario : website gives an api 
tricky sites: we pretend to be humans and get data

tools:
beautifulSoup package in python for simple html parsing
scrapy a framework in python for advanced and large scale projects
selenium and playwright for dynamic scraping and for interactive sites
playwright is a faster alternative to selenium

components of web scraping 
crawler: moves across web pages; follows links; finds target pages
scraper: extracts required data; based on tags classes etc

scraping vs crawling
scraping == extracting data; data focused
crawling == visits and indexes pages; discovery focused

Working of web scrapers:
url input 
http request is sent to website
html content is downloaded
parsed
only the required data is extracted
data is cleaned and stored

Python for web scraping : easy to learn and readable syntax
more libraries 
automation and data processing


challenges
website layout changes break scrapers
are you a robot or captcha blockers
legal and ethical issues
cleaning data is time consuming

Legal and ethical aspects
web scraping is not illegal by default
depends on website terms and usage
may violate copyright or terms of service
scraping personal data is unethical
overloading servers is unethical


Best Practices
rate limiting
no personal data scraping
prefer official apis
follow robots.txt rules
be transparent and ethical


future of ws:
more use of ai and automation
better handling of dynamic websites
more apis reducing need for scraping



API
date : 10\02\2026 && 11\02\2026
Topics done: 
HTTP basics
HTTP request & response structure
HTTP methods (GET, POST, PUT, PATCH, DELETE)
HTTP status codes (2xx, 3xx, 4xx, 5xx)
HTTP headers (request & response)
Cookies & session handling
URL structure
Path parameters
Query parameters
Basic TCP/IP understanding
What is API
Why API is better than scraping HTML
Types of APIs (Public, Private, Partner)
API architectural styles (REST, SOAP, GraphQL, gRPC)
Difference between API styles
Session identity (headers + cookies + IP)
Proxy concept (IP rotation)
Inspecting network tab for real requests
Observing request headers & response headers live
Checking JSON responses in XHR/fetch calls

=====================================================================

basic flow (scraping view)

browser or scraper sends request
website sends response

request has : method + url + headers + cookies + body(optional)
response has : status + headers + data(html/json)

checked this in inspect : network

----------------------------------------------------------------

HTTP

http = rulebook of web communication
every scrape is an http request

scraper behaves like browser

-------------------------------------------------

HTTP methods

GET : most scraping
used to load pages and data

POST : forms, login, search, filters

PUT PATCH DELETE : mostly api side, not common in scraping

saw GET and POST mostly in network tab

----------------------------------------------------------

HTTP status codes (scraping challengers)

200 : ok data received
301 : redirect
304 : cached
403 : blocked
404 : wrong url
429 : too many requests
500 : server error

403 and 429 are common in scraping

---------------------------------------------------------------------

headers (very important)

headers = identity hints

request headers like:
user-agent
accept
accept-language
referer
cookie

response headers like:
content-type
set-cookie
cache-control

copied headers from browser to scraper
saw all this in inspect

-----------------------------------------------------------------------------

cookies

server sends cookie in response
browser sends cookie back in next request

cookie = session identity

without cookie
login breaks
session resets

checked cookies in Application tab


session understanding

session = one consistent identity

built using:
headers
cookies
IP

if one changes : server suspicious

diagram made this clear


IP address

website tracks IP
too many requests : temp block

proxy rotates IP
so scraping continues

IP + cookie sometimes linked
changing IP may break session


proxy (scraping use)

proxy hides real IP
rotates IP if blocked

used when scraping large scale


URL, path & query parameters

path param : part of url
example: /product/123

query param : after ?
example: ?page=2

scrapers change query parameters
to move pagination

tested this manually in browser


TCP/IP (just enough)

IP = address
TCP = reliable connection

http runs on tcp

no deep networking needed for scraping


what is API (scraping angle)

api = clean data access
returns json

better than scraping html

best case : website exposes api

found api calls in inspect : XHR tab


api types

public : open
private : internal
partner : limited


api styles (scraping relevance)

REST : most common
json + http methods

SOAP : xml heavy
rare

GraphQL : flexible
single endpoint

gRPC : fast, binary
microservices

for scraping : REST APIs most useful


important scraping takeaway

scraping is not just request sending

it is about:
looking like browser
maintaining session
not triggering blocks









ADVANCED SCRAPING BASICS

date : 12/02/2026
Topics done:
User-Agent strings; browser fingerprint idea; DevTools network usage; Requests library; curl-cffi; browser impersonation; headers cloning; anti-bot basics.
===========================================================================================================================================================

User-Agent strings

user-agent = identity string sent in request header

tells website:
which browser
which OS
which device

example:
Mozilla/5.0 (Windows NT 10.0; Win64; x64) 

if scraper sends no user-agent
server may block

copied real user-agent from inspect : request headers

deviceatlas site showed different UA examples
mobile UA different from desktop UA

changing UA changes how site responds sometimes

--------------------------------------------------------------

why user-agent matters in scraping

some websites block python default user-agent
like: python-requests/2.x

so we replace it with real browser UA

makes scraper look human

not full bypass but basic layer

-------------------------------------------------------

Browser DevTools 

opened inspect : Network tab

observed:
request method
status code
request headers
response headers
payload
response preview

filtered by:
Fetch/XHR

found API endpoints behind UI

clicked each request
copied headers
copied cookies

realized many sites load data via API not HTML

DevTools = best reverse engineering tool

----------------------------------------------------------

Network observation learnings

some requests triggered only when scrolling
some triggered on button click

means dynamic loading

saw JSON responses directly

realized scraping HTML sometimes unnecessary
better hit API directly

----------------------------------------------------------------

Requests library (python)

requests = simple HTTP client

used for:
GET
POST
sending headers
sending cookies
handling sessions

very readable syntax

example mental flow:

requests.get(url, headers=..., cookies=...)

supports:
timeout
proxies
sessions

Requests calls itself: HTTP for Humans

default UA is python-requests
so override required

-------------------------------------------------------------------

Sessions in Requests

requests.Session()

maintains cookies automatically

simulates browser session

important for login scraping

without session
each request isolated

-------------------------------------------------------

curl-cffi

browser impersonating http client

uses real browser TLS fingerprint

harder to detect

normal requests only changes headers
but TLS fingerprint still different

curl-cffi tries to mimic real browser at lower level

important for advanced anti-bot bypass

------------------------------------------------------------------

basic difference

Requests : header level mimic
curl-cffi : deeper browser mimic

when site very strict
curl-cffi better

-------------------------------------------------------

browser impersonation concept

anti-bot systems check:

headers
user-agent
TLS fingerprint
IP reputation
cookie behavior

so scraping is not just headers

impersonation tries to match real browser behavior

--------------------------------------------------------------------

fingerprint idea (basic understanding)

browser has fingerprint

based on:
headers
TLS
timing
js execution

headless browser sometimes detected

so sometimes real browser automation used

-----------------------------------------------------------------

DevTools practical things I checked

changed user-agent manually
refreshed page

modified query params

blocked cookies and saw login fail

checked response headers for rate limit hints

observed 304 responses when cached

-------------------------------------------------------------------------

anti-bot basics understood

403 : blocked
429 : too many requests
captcha triggered after repeated calls

solution ideas:

rate limiting
proxy rotation
session persistence
real headers

---------------------------------------------------------------------

imp notes

scraping = network observation + behavior mimic

DevTools teaches more than theory

user-agent only not enough
identity = headers + cookies + IP + TLS

---------------------------------------------------------------










SELENIUM
date : 13/02/2026

Selenium is a browser automation tool.
It is not just one single tool, it is a project with multiple components.

Main purpose: automate web browsers.
It can open browser, click buttons, type text, scroll, submit forms etc.
Works like a real user using the browser.

Core part of Selenium is WebDriver.
WebDriver sends instructions to the browser.
It follows W3C standard so same code works for Chrome, Firefox, Edge etc.

Basic flow understood:
create driver
open browser
go to website
perform actions
close browser

Selenium is useful when websites are dynamic and use a lot of JavaScript.
Requests library cannot handle JavaScript execution, but Selenium can because it controls real browser.

Main components:

WebDriver
Main automation engine.
Controls browser natively.

Selenium IDE
Browser extension.
Records user actions.
Good for beginners to learn commands.

Selenium Grid
Used to run tests on multiple machines and browsers.
Helpful for large scale testing.

Selenium Manager
Automatically manages browser drivers.
No need to manually download drivers in most cases.

Difference from requests:
requests works at HTTP level.
Selenium works at browser level.
Selenium is slower but more powerful for dynamic sites.

Overall understanding:
Selenium is mainly used for browser automation and testing.
Also helpful in scraping when site is heavily dynamic.
